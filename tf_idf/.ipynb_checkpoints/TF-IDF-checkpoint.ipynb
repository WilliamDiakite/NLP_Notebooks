{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing TF-IDF\n",
    "<br>\n",
    "TF-IDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Getting some data\n",
    "Let's start by setting up three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "documents.append(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard Clabaugh. \n",
    "The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "\n",
    "documents.append(\"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\")\n",
    "\n",
    "documents.append(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Implementing a tokenizer\n",
    "\n",
    "Before counting words, we want to lemmatize everything. This step helps to reduce dimensionnality of our data. \n",
    "<br>\n",
    "Here's simple tokenizer/lemmatizer using NLTK :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = [l.rstrip() for l in open('stopwords.txt').read()]\n",
    "\n",
    "def tokenize(document):\n",
    "    # Put everything to lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Split document into words (=tokens) \n",
    "    tokens = word_tokenize(document)\n",
    "    \n",
    "    # Remove all words that are too small\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    \n",
    "    # Remove all stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    \n",
    "    # Turn tokens into their base form\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # Removes all digits\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Tokenize all documents\n",
    "all_tokens = [tokenize(d) for d in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Computing TF (Term-frequency)\n",
    "\n",
    "Here, we're going to compute the term-frequency matrix which is defined as follows: $tf_{i,j} = \\frac{n_{i,j}}{\\sum_{k} n_{k,j}}$\n",
    "<br>\n",
    "Where i is the index of ith word, j is the index of the current document, and $\\sum_{k} n_{k,j}$ is the total of words in that document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to give every token an unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'described': 0, 'now': 1, 'martin': 2, 'wheaton': 3, 'role': 4, 'classic': 5, 'van': 6, 'directed': 7, 'malibu': 8, 'girl': 9, 'coogan': 10, 'ever': 11, 'have': 12, 'engineered': 13, 'combat': 14, 'snake': 15, 'jenny': 16, 'genus': 17, 'ian': 18, 'movie': 19, 'keith': 20, 'several': 21, 'leroy': 22, 'wesson': 23, 'word': 24, 'colt': 25, 'smeets': 26, 'krueger': 27, 'casper': 28, 'targeted': 29, 'also': 30, 'bowe': 31, 'his': 32, 'two': 33, 'python': 34, 'elm': 35, 'escape': 36, 'nightmare': 37, 'asia': 38, 'town': 39, 'jeff': 40, 'sean': 41, 'made-for-tv': 42, 'smith': 43, 'nonvenomous': 44, 'found': 45, 'fame': 46, 'renee': 47, 'street': 48, 'feature': 49, 'specie': 50, 'boa': 51, 'reticulatus': 52, 'hartford': 53, 'mccarthy': 54, 'same': 55, 'william': 56, 'made': 57, 'πύθων/πύθωνας': 58, 'sometimes': 59, 'for': 60, 'wil': 61, 'whalen': 62, 'itself': 63, 'connecticut': 64, 'manufactured': 65, 'known': 66, 'los': 67, 'sequel': 68, 'collector': 69, 'revolver': 70, 'year': 71, 'best': 72, 'angeles': 73, 'premium': 74, 'wa': 75, 'clabaugh': 76, 'horror': 77, 'magnum': 78, 'series': 79, 'david': 80, 'that': 81, 'richard': 82, 'amp': 83, 'includes': 84, 'the': 85, 'caliber': 86, 'friday': 87, 'both': 88, 'segment': 89, 'from': 90, 'evident': 91, 'recognised': 92, 'member': 93, 'firearm': 94, 'africa': 95, 'actor': 96, 'currently': 97, 'this': 98, 'like': 99, 'favorite': 100, 'robert': 101, 'concern': 102, 'filmed': 103, 'small': 104, 'englund': 105, 'cooper': 106, 'freddy': 107, 'zabka': 108, 'scenario': 109, 'among': 110, 'referred': 111, 'cult': 112, 'discontinued': 113, 'finest': 114, 'greek': 115, 'film': 116, 'dana': 117, 'genetically': 118, 'production': 119, 'formerly': 120, 'followed': 121, 'hawk': 122, 'california': 123, 'final': 124, 'kid': 125, 'such': 126, 'introduced': 127, 'karate': 128, 'manufacturing': 129, 'some': 130, 'and': 131, 'dien': 132, 'hogg': 133, 'dougherty': 134, 'thompson': 135, 'unleashes': 136, 'longest': 137, 'barron': 138, 'writer': 139, 'first': 140, 'are': 141, 'chuck': 142, 'market': 143, 'vs.': 144, 'company': 145, 'including': 146}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Get all tokens in a single array and removes duplicates \n",
    "vocabulary = list(set(itertools.chain.from_iterable(all_tokens)))\n",
    "\n",
    "# Dict that maps word: index\n",
    "word_index_map = {w: i for i, w in enumerate(vocabulary)}\n",
    "\n",
    "print(word_index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now let's cmpute TF for every documents:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(all_tokens)\n",
    "W = len(vocabulary)\n",
    "\n",
    "tf = np.zeros((D, W))\n",
    "for i in range(D):\n",
    "    total_words = len(all_tokens[i])\n",
    "    for t in all_tokens[i]:\n",
    "        tf[i,word_index_map[t]] += 1/total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Computing IDF ( Inverse Document Frequency)\n",
    "\n",
    "Contrary to the TF frequency matrix, all document are used to compute the IDF. The IDF is computed as follows: $idf_{j} = log(\\frac{nb_{document}}{nb_{presence}}) $ \n",
    "<br>\n",
    "where j is the index of the current word $w_j$, $nb_{document}$ is number of document that we are dealing with (3, in our case) and $nb_{presence}$ is the number of document where $w_j$ appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf = np.zeros(W)\n",
    "for token in vocabulary:\n",
    "    for tokens in all_tokens:\n",
    "        if token in tokens:\n",
    "            idf[word_index_map[token]] += 1\n",
    "\n",
    "idf = [math.log(D/i) for i in idf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Computing TF-IDF\n",
    "\n",
    "The TF-IDF of a word $w_j$ from a particular document $i$ is computed by multiplying its $tf_{i,j}$ with $idf_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf = tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Finding the top words of a document\n",
    "\n",
    "For each document we would like to see what are the top words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for document 1\n",
      "\tWord: film, TF-IDF: 0.05549\n",
      "\tWord: made-for-tv, TF-IDF: 0.02219\n",
      "\tWord: california, TF-IDF: 0.02219\n",
      "Top words for document 2\n",
      "\tWord: genus, TF-IDF: 0.08451\n",
      "\tWord: word, TF-IDF: 0.04225\n",
      "\tWord: asia, TF-IDF: 0.04225\n",
      "Top words for document 3\n",
      "\tWord: colt, TF-IDF: 0.04919\n",
      "\tWord: revolver, TF-IDF: 0.04919\n",
      "\tWord: magnum, TF-IDF: 0.04919\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "\n",
    "\n",
    "for i in range(D):\n",
    "    # Creates a map between words and their TF-IDF scores (words: tfidf)\n",
    "    scores = {w: tf_idf[i, word_index_map[w]] for w in vocabulary}\n",
    "    scores = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    print(\"Top words for document {}\".format(i+1))\n",
    "    for score in scores[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(score[0], round(score[1], 5)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
