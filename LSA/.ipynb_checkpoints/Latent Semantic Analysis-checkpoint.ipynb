{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latente Semantic Analysis (LSA)\n",
    "\n",
    "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Words are then compared by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by any two rows. Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Compute TF-ID \n",
    "\n",
    "cf TF-IDF notebook for more informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "\n",
    "# Get some data\n",
    "documents = []\n",
    "\n",
    "documents.append(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard Clabaugh. \n",
    "The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "\n",
    "documents.append(\"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\")\n",
    "\n",
    "documents.append(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\")\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = [l.rstrip() for l in open('stopwords.txt').read()]\n",
    "\n",
    "def tokenize(document):\n",
    "    # Put everything to lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Split document into words (=tokens) \n",
    "    tokens = word_tokenize(document)\n",
    "    \n",
    "    # Remove all words that are too small\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    \n",
    "    # Remove all stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    \n",
    "    # Turn tokens into their base form\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    # Removes all digits\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Tokenize all documents\n",
    "all_tokens = [tokenize(d) for d in documents]\n",
    "\n",
    "# Get all tokens in a single array and removes duplicates \n",
    "vocabulary = list(set(itertools.chain.from_iterable(all_tokens)))\n",
    "\n",
    "# Dict that maps word: index\n",
    "word_index_map = {w: i for i, w in enumerate(vocabulary)}\n",
    "\n",
    "# Compute TF\n",
    "D = len(all_tokens)\n",
    "W = len(vocabulary)\n",
    "tf = np.zeros((D, W))\n",
    "\n",
    "for i in range(D):\n",
    "    total_words = len(all_tokens[i])\n",
    "    for t in all_tokens[i]:\n",
    "        tf[i,word_index_map[t]] += 1/total_words\n",
    "        \n",
    "# Compute IDF \n",
    "idf = np.zeros(W)\n",
    "for token in vocabulary:\n",
    "    for tokens in all_tokens:\n",
    "        if token in tokens:\n",
    "            idf[word_index_map[token]] += 1\n",
    "\n",
    "idf = [math.log(D/i) for i in idf]\n",
    "\n",
    "\n",
    "\n",
    "#--- Compute TF-IDF\n",
    "tf_idf = tf * idf\n",
    "\n",
    "#--- Transpose for SVD \n",
    "tf_idf = tf_idf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
